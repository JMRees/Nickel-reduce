{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nickel Direct Imaging Data Reduction\n",
    "\n",
    "Basis of this jupyter notebook is from Keerthi Vasan Gopala Chandrasekaran (UC-Davis), who created it from Elinor Gates' (UCO/Lick) 2018 Observational Astronomy Workshop python data reduction activity. Additional code contributions and conversion so it would work under Python 3 were from Azalee Bostroem (UC-Davis).  Elinor Gates subsequently added commentary, expanded the code to make sure everything is done inside python, and added the cosmic ray rejection section. Jon Rees modified things further and added the photometry section. This is designed to work with Python 3.   \n",
    "\n",
    "If the data are properly acquired and FITS headers are accurate, this should work as a basic data reduction pipeline. However, proceeding slowly, one step at a time, examining calibration and image frames at each step is encouraged so that understanding of each step and its importance to the general data reduction is understood, as well as catching errors and implementing fixes as soon as possible in the procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Necessary Python Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits,ascii\n",
    "import numpy as np\n",
    "import sys, getopt,os\n",
    "from glob import glob\n",
    "import math\n",
    "# shutil is used for the file copying\n",
    "import shutil\n",
    "# tqdm gives us a handy progress bar for some of the more time consuming steps\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with Astroscrappy\n",
    "\n",
    "Later in the notebook we'll use astroscrappy to deal with cosmic ray removal. \n",
    "This cell will install astroscrappy if it is not already installed.\n",
    "If the cell runs without errors, you're (probably) good to go.\n",
    "\n",
    "If Astroscrappy segfaults later in this notebook, try removing it before re-running the below code (pip uninstall -y astroscrappy)\n",
    "\n",
    "If you're running this on Windows, you'll need Microsoft's Visual C++ Build Tools:\n",
    "https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "\n",
    "If you're running on Windows Subsystem for Linux you'll need to install gcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import astroscrappy\n",
    "    print(\"module 'astroscrappy' is installed\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"module 'astroscrappy' is not installed\")\n",
    "    !{sys.executable} -m pip install astroscrappy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organise Data\n",
    "\n",
    "Everyone will have their own preferred method of organising their data. As the current arbiter of this reduction activity, the notebook has been written to conform to my preferred method: Original data files should never be overwritten/altered. You should be able to go back and re-run the reduction multiple times from your original files as you learn new quirks of the data.\n",
    "\n",
    "We will set the path to the parent directory below using the variable source_dir.\n",
    "\n",
    "\n",
    "<section class=\"challenge panel panel-success\"> \n",
    "<div class=\"panel-heading\">\n",
    "<font color='green'>\n",
    "<h2><span class=\"fa fa-pencil\"></span> Set up initial directories </h2>\n",
    "</font>    \n",
    "**Create two directories, 'Data' and 'Reduced'. Place your data for the given night inside the 'Data' directory.**\n",
    "    \n",
    "</div>\n",
    "</section>\n",
    "\n",
    "\n",
    "<p>\n",
    "<img src=./images/ParentFolderStructure.png>\n",
    "<p>\n",
    "\n",
    "The initial directory structure should include a 'Data' directory and a 'Reduced' directory. The initial data files will be copied from 'Data' to 'Reduced', and all subsequent operations will be performed on the data in the 'Reduced' directory. If you need to re-run the reduction, you can delete the files in the 'Reduced' directory without worry.\n",
    "\n",
    "Below we automatically set up directories for file sorting inside the Reduced directory:\n",
    "<p>\n",
    "<img src=./images/FolderStructure.png>\n",
    "<p>\n",
    "\n",
    "<section class=\"challenge panel panel-success\"> \n",
    "<div class=\"panel-heading\">\n",
    "<font color='green'>\n",
    "<h2><span class=\"fa fa-pencil\"></span> Change Source Directory</h2>\n",
    "</font>\n",
    "    \n",
    "**You'll want to change the source directory appropriately for your data location.**\n",
    "    \n",
    "</div>\n",
    "</section>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The source directory. Set this to wherever your Data and Reduced directories live.\n",
    "source_dir = '/home/jrees/DataReduction/20230720/'\n",
    "\n",
    "# Location of the folder containing all the data files (bias, domeflats, twilight flats, and the data files)\n",
    "data_dir = source_dir + 'Data/'\n",
    "redu_dir = source_dir + 'Reduced/'\n",
    "\n",
    "# Check that the data directory actually exists\n",
    "if os.path.exists(data_dir) == False:\n",
    "    raise ValueError(\"Data directory does not exist\")\n",
    "# And make sure that it is not empty\n",
    "if len(os.listdir(data_dir)) == 0:\n",
    "    raise ValueError(\"Data directory is empty\")\n",
    "    \n",
    "# Make some directories to organise files by type (bias, flats etc.)\n",
    "# The archive directory will store files that are no longer needed for the data reduction, \n",
    "# but still available to examine if needed if there are issues with the data reduction.  \n",
    "\n",
    "biasdir = redu_dir+'Bias/'\n",
    "datadir = redu_dir+'Data_files/'\n",
    "domeflatdir = redu_dir+'Flat_dome/'\n",
    "twiflatdir = redu_dir+'Flat_twilight/'\n",
    "archivedir = redu_dir+'Archive/'\n",
    "\n",
    "\n",
    "os.makedirs(biasdir,exist_ok=True)\n",
    "os.makedirs(datadir,exist_ok=True)\n",
    "os.makedirs(domeflatdir,exist_ok=True)\n",
    "os.makedirs(twiflatdir,exist_ok=True)\n",
    "os.makedirs(archivedir,exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Data\n",
    "\n",
    "Now we copy the initial data to the Reduced directory. This way our original data remains safe, and we can always easily reproduce what we did to reduce the data.\n",
    "\n",
    "\n",
    "<section class=\"challenge panel panel-success\"> \n",
    "<div class=\"panel-heading\">\n",
    "<font color='green'>\n",
    "<h2><span class=\"fa fa-pencil\"></span> Set any files to be removed</h2>\n",
    "</font>\n",
    "    \n",
    "**If you have any bad data frames, you can remove them by adding the frame numbers to delfilelist and setting delfiles = 'yes'**\n",
    "\n",
    "</div>\n",
    "</section>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all of the data from the Data directory to the Reduction directory\n",
    "# Our input list is just all of the FITS files in the Data directory\n",
    "ifilelist = glob(data_dir+'*.fits')\n",
    "# Our output location is the Reduced direcory so doesn't need a list\n",
    "\n",
    "# Copy the files using shutil\n",
    "for file in ifilelist:\n",
    "    shutil.copy2(file, redu_dir)\n",
    "\n",
    "# And if you want to remove any known-bad files, add them to the list below and set delfiles = yes\n",
    "delfiles = 'no'\n",
    "\n",
    "if delfiles == 'yes':\n",
    "    # Set the frame numbers of the frames to delete from the Reduced directory\n",
    "    delfilelist = ('d1036', 'd1037', 'd1041', 'd1045')\n",
    "    for file in delfilelist:\n",
    "        # Check if the file exists\n",
    "        if os.path.isfile(redu_dir + file + '.fits'):\n",
    "        # If it does exist, delete it\n",
    "            os.remove(redu_dir + file + '.fits')\n",
    "            print(\"Deleting FITS file \" + file + '.fits' + \" from Reduced direcory.\")\n",
    "            print(\"-----------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overscan Subtraction\n",
    "\n",
    "The overscan region(s) are additional columns appended to the data that measure the overall bias values for each row at the time the data were acquired.  Depending on the camera, these may be very stable over a night of observing, or vary from image to image. This bias level needs to be subtracted before further data reduction steps should be done.\n",
    "Overscan subtraction is done on all calibration and science files.  \n",
    "\n",
    "The original overscanLickObsP3.py code is available on-line via our optical instrument manuals will read a list of files in, determine the overscan and data regions for each file, fit the overscan, then subtract it from the data, writing out a new overscan subtracted image for each input image. This code is specific to Lick Observatory data and keywords, but could easily be altered with the appropriate keywords to work with other detectors with one or two amplifiers.  The code below has been modified from the original to create input and output filelists according to the file directory denoted above. If you have a lot of data, this may take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set fit = 'yes' to do legendre fit to overscan regions, 'no' to just use the median\n",
    "fit = 'yes' \n",
    "    \n",
    "# for i in range(0,numifiles):\n",
    "#        ifile=ifilelist[i]\n",
    "#        basename=os.path.basename(ifile)\n",
    "#        print(basename)    \n",
    "    \n",
    "# Our input file list is everything in the Reduced directory\n",
    "ifilelist = glob(redu_dir+'*.fits')\n",
    "\n",
    "# For each file in ifilelist, we need to read in the file,  \n",
    "# figure out overscan and data regions, fit the overscan with desired function (if any),  \n",
    "# subtract the overscan from the data, and finally write the data to an output file.\n",
    "\n",
    "for ifile in tqdm(ifilelist):\n",
    "    # The output files will have _os appended (overscan subtracted) in their file names\n",
    "    ofile = ifile[:-5]+ '_os.fits'\n",
    "    # Read in the input FITS file using the fits module from astropy.io\n",
    "    data, header = fits.getdata(ifile,header=True)\n",
    "    # Change data to float\n",
    "    data=data.astype('float32')\n",
    "    \n",
    "    # read necessary keywords from fits header\n",
    "\n",
    "    #number of pixels in image\n",
    "    xsize = header['NAXIS1']\n",
    "    ysize = header['NAXIS2']\n",
    "    #start column and row\n",
    "    xorig = header['CRVAL1U']\n",
    "    yorig = header['CRVAL2U']\n",
    "    #binning and direction of reading pixels\n",
    "    cdelt1 = header['CDELT1U']\n",
    "    cdelt2 = header['CDELT2U']\n",
    "    # number of overscan rows/columns\n",
    "    rover = header['ROVER']\n",
    "    cover = header['COVER']\n",
    "    #unbinned detector size\n",
    "    detxsize = header['DNAXIS1']  \n",
    "    detysize = header['DNAXIS2']\n",
    "    #number of amplifiers\n",
    "    ampsx = header['AMPSCOL']\n",
    "    ampsy = header['AMPSROW']\n",
    "\n",
    "    # determine number and sizes of overscan and data regions\n",
    "    namps = ampsx*ampsy\n",
    "    if rover > 0:\n",
    "        over=rover\n",
    "        sys.exit('Program does not yet deal with row overscans. Exiting.')\n",
    "    else:\n",
    "        over = cover\n",
    "    if over == 0:\n",
    "        sys.exit('No overscan region specified in FITS header. Exiting.')\n",
    "\n",
    "    # single amplifier mode (assumes overscan is the righmost columns)\n",
    "    if namps == 1:\n",
    "        biassec = data[:,xsize-cover:xsize]\n",
    "        datasec = data[0:,0:xsize-cover]\n",
    "\n",
    "        # median overscan section\n",
    "        bias=np.median(biassec, axis=1) \n",
    "\n",
    "        # legendre fit\n",
    "        if fit == 'yes':\n",
    "            # fit\n",
    "            lfit = np.polynomial.legendre.legfit(range(0,len(bias)),bias,3)\n",
    "            bias = np.polynomial.legendre.legval(range(0,len(bias)),lfit)\n",
    "\n",
    "        # subtract overscan\n",
    "        datanew = datasec\n",
    "        for i in range(datasec.shape[1]):\n",
    "            datanew[:,i] = datasec[:,i]-bias\n",
    "\n",
    "    # two amplifier mode (assumes both amplifer overscans are at rightmost columns)\n",
    "    if namps == 2:\n",
    "        biasseca = data[:,xsize-cover*2:xsize-cover]\n",
    "        biassecb = data[:,xsize-cover:xsize]\n",
    "\n",
    "        # median overscan sections\n",
    "        biasa=np.median(biasseca,axis=1)\n",
    "        biasb=np.median(biassecb,axis=1)\n",
    "\n",
    "        # legendre fit\n",
    "        if fit == 'yes':\n",
    "            lfita = np.polynomial.legendre.legfit(range(0,len(biasa)),biasa,3)\n",
    "            lfitb = np.polynomial.legendre.legfit(range(0,len(biasb)),biasb,3)\n",
    "            biasa = np.polynomial.legendre.legval(range(0,len(biasa)),lfita)\n",
    "            biasb = np.polynomial.legendre.legval(range(0,len(biasb)),lfitb)\n",
    "\n",
    "        # Extract data regions\n",
    "\n",
    "        # determine boundary between amplifiers\n",
    "        bd=detxsize/2/abs(cdelt1)\n",
    "\n",
    "        # calculate x origin of readout in binned units if cdelt1 negative or positive\n",
    "        if cdelt1 < 0:\n",
    "            #if no binning x0=xorig-xsize-2*cover, with binning:\n",
    "            x0=xorig/abs(cdelt1)- (xsize-2*cover) \n",
    "        else:\n",
    "            x0=xorig/cdelt1\n",
    "                \n",
    "        xtest=x0+xsize-cover*2 # need to test if all data on one or two amplifiers\n",
    "\n",
    "        # determine which columns are on which amplifier and subtract proper overscan region\n",
    "\n",
    "        if xtest < bd: # all data on left amplifier\n",
    "            datanew=data[:,0:xsize-cover*2]\n",
    "            m=datanew.shape[1]\n",
    "            for i in range(0,m):\n",
    "                datanew[:,i]=datanew[:,i]-biasa\n",
    "\n",
    "        if x0 >= bd: # all data on right amplifier\n",
    "            datanew=data[:,0:xsize-cover*2]\n",
    "            m=datanew.shape[1]\n",
    "            for i in range(0,m):\n",
    "                datanew[:,i]=datanew[:,i]-biasb\n",
    "\n",
    "        if xtest >= bd and x0 < bd:  #data on both amplifiers\n",
    "            x1=int(bd-x0)\n",
    "            dataa=data[:,0:x1]\n",
    "            datab=data[:,x1:-cover*2]\n",
    "            ma=dataa.shape[1]\n",
    "            mb=datab.shape[1]\n",
    "            for i in range(0,ma):\n",
    "                dataa[:,i]=dataa[:,i]-biasa\n",
    "            for i in range(0,mb):\n",
    "                datab[:,i]=datab[:,i]-biasb\n",
    "            # merge dataa and datab into single image\n",
    "            datanew=np.hstack([dataa,datab])\n",
    "                \n",
    "    if namps > 2:\n",
    "        sys.exit('Program does not yet deal with more than two overscan regions. Exiting.')\n",
    "\n",
    "    # add info to header\n",
    "    header['HISTORY'] = 'Overscan subtracted'\n",
    "\n",
    "    # write new fits file\n",
    "    fits.writeto(ofile,datanew,header,overwrite=True)\n",
    "    # And move the input file to the archive directory\n",
    "    basename=os.path.basename(ifile)\n",
    "    os.rename(ifile,archivedir+basename)\n",
    "    \n",
    "# When done with the subtraction, let us know\n",
    "print(\"Overscan subtraction completed.\")\n",
    "print(\"-----------------------------------------------------\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organise Overscan Subtracted Files  \n",
    "\n",
    "Move all the overscan subtracted (e.g. the newly created *_os.fits) bias, data, and flat field files into separate folders, based upon the OBJECT field in the FITS headers.\n",
    "\n",
    "<section class=\"challenge panel panel-success\"> \n",
    "<div class=\"panel-heading\">\n",
    "<font color='green'>\n",
    "<h2><span class=\"fa fa-pencil\"></span> Check Flat Field headers</h2>\n",
    "</font>\n",
    "    \n",
    "**If your flat field OBJECT field does not use dome/twi for dome flats and twilight flats respectively, you will need to update the 'if' loops below.**\n",
    "\n",
    "</div>\n",
    "</section>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of all the overscan subtracted files\n",
    "os_files = glob(redu_dir+'*os.fits')\n",
    "\n",
    "# Move calibration frames to appropriate directories\n",
    "# In this case we are assuming that twilight flats have 'twi' in the OBJECT FITS header keyword, \n",
    "# dome flats have 'dome' in OBJECT, etc.  If the names are different, you'll need to adjust the search strings\n",
    "# for sorting the files.\n",
    "for ifile in os_files:\n",
    "    hdr = fits.getheader(ifile)\n",
    "    basename=os.path.basename(ifile)\n",
    "    if 'twi' in hdr['OBJECT'].lower():\n",
    "        os.rename(ifile,twiflatdir+basename)\n",
    "    elif 'dome' in hdr['OBJECT'].lower():\n",
    "        os.rename(ifile,domeflatdir+basename)\n",
    "    elif 'bias' in hdr['OBJECT'].lower():\n",
    "        os.rename(ifile,biasdir+basename)\n",
    "    else:\n",
    "        os.rename(ifile,datadir+basename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Master Bias File \n",
    "\n",
    "The Master Bias file is the median combined bias frames.  If the detector is particularly flat with no bias structure, this step may not be needed.  In the case of the Nickel Direct Imaging CCD, there is significant bias structure that needs to be removed, so this step is necessary to remove that structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create list of bias files\n",
    "biasfiles = glob(biasdir + '*.fits')\n",
    "\n",
    "data_stack = []\n",
    "for file in biasfiles:\n",
    "    data_stack.append(fits.getdata(file))\n",
    "\n",
    "    \n",
    "# Median combine the bias files to create the Master Bias frame\n",
    "medianBias = np.median(data_stack,axis=0)\n",
    "\n",
    "# Write out the master bias file with updated FITS header information\n",
    "header = fits.getheader(biasfiles[0])\n",
    "header['HISTORY'] = 'Median combined'\n",
    "fits.writeto(redu_dir+'bias.fits',medianBias,header)  \n",
    "# For Windows machines we have to reset the data stack, otherwise it keeps the bias\n",
    "# files open and we can't move them\n",
    "data_stack = []\n",
    "# Move the no longer needed overscan subtracted frames to the archive directory\n",
    "for file in biasfiles:\n",
    "    basename=os.path.basename(file)\n",
    "    os.rename(file,archivedir+basename)\n",
    "\n",
    "print(\"Created Master Bias frame.\")\n",
    "print(\"-----------------------------------------------------\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Master Bias File\n",
    "\n",
    "It is best to check the bias.fits file to make sure it looks OK before continuing.  DS9 is a frequently used tool in astronomy for examining FITS images.  DS9 is not a python tool, but freely downloadable for virtually all computer operating systems.  Typical Nickel bias images look like the following. <p>\n",
    "    <img src=./images/DS9-NickelBias.png>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Subtract Flat Field and Data Frames\n",
    "\n",
    "Because the files were sorted into subdirectories, we'll be doing essentially the same steps for the files in the Data_files, flat_dome, and flat_twilight directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias subtracting the data files\n",
    "\n",
    "# Make list of input files\n",
    "datafilesin = glob(datadir + '*.fits')\n",
    "\n",
    "for ifile in tqdm(datafilesin):\n",
    "    # _bs stands for bias subtracted in the output file names\n",
    "    ofile = ifile[:-5]+ '_bs.fits' \n",
    "    data,header = fits.getdata(ifile,header=True)\n",
    "    dataout = data - medianBias\n",
    "    header['HISTORY'] = 'Bias subtracted'\n",
    "    fits.writeto(ofile,dataout,header)\n",
    "    # Again, clear the arrays so Windows doesn't complain about open files\n",
    "    data = []\n",
    "    header = []\n",
    "    # Move the no longer needed overscan subtracted files to the archive directory\n",
    "    basename=os.path.basename(ifile)\n",
    "    os.rename(ifile,archivedir+basename)\n",
    "    \n",
    "print(\"Debiased Data frames.\")\n",
    "print(\"-----------------------------------------------------\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias subtracting the dome flat files\n",
    "\n",
    "# Make list of input dome flat field files\n",
    "datafilesin = glob(domeflatdir + '*.fits')\n",
    "\n",
    "for ifile in tqdm(datafilesin):\n",
    "    # _bs stands for bias subtracted in the output file names\n",
    "    ofile = ifile[:-5]+ '_bs.fits' \n",
    "    data,header = fits.getdata(ifile,header=True)\n",
    "    dataout = data - medianBias\n",
    "    header['HISTORY'] = 'Bias subtracted'\n",
    "    fits.writeto(ofile,dataout,header)\n",
    "    # Move the no longer needed overscan subtracted files to the archive directory\n",
    "    data = []\n",
    "    header = []\n",
    "    basename=os.path.basename(ifile)\n",
    "    os.rename(ifile,archivedir+basename)\n",
    "\n",
    "print(\"Debiased Dome Flat frames.\")\n",
    "print(\"-----------------------------------------------------\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias subtracting the twilight flat files\n",
    "\n",
    "# Make list of input twilight flat field files\n",
    "datafilesin = glob(twiflatdir + '*.fits')\n",
    "\n",
    "for ifile in tqdm(datafilesin):\n",
    "    # _bs stands for bias subtracted in the output file names\n",
    "    ofile = ifile[:-5]+ '_bs.fits' \n",
    "    data,header = fits.getdata(ifile,header=True)\n",
    "    dataout = data - medianBias\n",
    "    header['HISTORY'] = 'Bias subtracted'\n",
    "    fits.writeto(ofile,dataout,header)\n",
    "    # Move the no longer needed overscan subtracted files to the archive directory\n",
    "    data = []\n",
    "    header = []\n",
    "    basename=os.path.basename(ifile)\n",
    "    os.rename(ifile,archivedir+basename)\n",
    "    \n",
    "print(\"Debiased Twilight Flat frames.\")\n",
    "print(\"-----------------------------------------------------\") \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Normalised Flat Field frames\n",
    "\n",
    "For this example we will use the twilight flat field frames, as they are generally superior to dome flats.  One uses dome flats if the twilight flat field frames were unattainable due to weather or there was some other technical issues.  First we will create lists of files for each filter, then combine the frames to create the final normalised flat field frame for each filter.\n",
    "\n",
    "<section class=\"challenge panel panel-success\"> \n",
    "<div class=\"panel-heading\">\n",
    "<font color='green'>\n",
    "<h2><span class=\"fa fa-pencil\"></span> Choose Twilight Flats or Dome Flats and set the correct Filters</h2>\n",
    "</font>\n",
    "    \n",
    "**If you need to use Dome Flats instead of Twilight Flats, change flat_dir to reflect this.**\n",
    "    \n",
    "**By default we assume B, V, R, I filters were used. Update them below if you used a different set.**\n",
    "\n",
    "</div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes that the B, V, R, and I filters were used.  If different filters were used, you'll need to change the \n",
    "# code below accordingly\n",
    "\n",
    "# If you are using dome flat fields, rather than twilight flats, change the below to domeflatdir\n",
    "flat_dir = twiflatdir\n",
    "# If you are using different filters, or a different number of filters, set them below\n",
    "filters = ['B','V','R','I']\n",
    "\n",
    "print(\"Using Filters: \")\n",
    "print (filters)\n",
    "if flat_dir == twiflatdir:\n",
    "    print(\"Using Flat Field frames from Twilight Flat directory.\")\n",
    "elif flat_dir == domeflatdir:\n",
    "    print(\"Using Flat Field frames from Dome Flat directory.\")\n",
    "else:\n",
    "    print(\"Unrecognised Flat Field directory\")\n",
    "print(\"-----------------------------------------------------\") \n",
    "\n",
    "# Make list of all the flat field files in the flat field directory\n",
    "flatlist = glob(flat_dir + '*.fits')\n",
    "\n",
    "# Our file lists will be contained in a dictionary called flist\n",
    "flist = {}\n",
    "\n",
    "for filter in filters:\n",
    "    # Create an empty list for the file names\n",
    "    flist[filter] = []\n",
    "\n",
    "# Sort files into lists based on the filter used\n",
    "for ifile in flatlist:\n",
    "    # Read the header for each flat file\n",
    "    hdr = fits.getheader(ifile)\n",
    "    # Read which filter this file was taken with\n",
    "    filt = hdr['FILTNAM']\n",
    "    # Loop through each of the filters in our filter set\n",
    "    for filter in filters:\n",
    "        # If the filter listed in the header matches the filter for this array, add the file to the array\n",
    "        if filt == filter:\n",
    "            flist[filter].append(ifile)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each filter, we're going to create a Master Flat.\n",
    "# We'll use another dictionary to keep track of the data stacks for each filter, \n",
    "# and one to store the normalised flat\n",
    "flat_stack = {}\n",
    "flat = {}\n",
    "\n",
    "for filter in filters:\n",
    "    # Initialise the stack for this filter\n",
    "    flat_stack[filter] = []\n",
    "    flat[filter] = []\n",
    "    \n",
    "    # Read in each file in this filter and divide by the median to normalise\n",
    "    for file in flist[filter]:\n",
    "        data,header = fits.getdata(file,header=True)\n",
    "        data = data / np.median(data)\n",
    "        # Append the data to the stacked data\n",
    "        flat_stack[filter].append(data)\n",
    "        # Move the now no longer needed files to archivedir\n",
    "        basename=os.path.basename(file)\n",
    "        os.rename(file,archivedir+basename)\n",
    "        \n",
    "    # Median combine the flat fields\n",
    "    flat[filter] = np.median(flat_stack[filter],axis=0)\n",
    "    # And divide by the mean to normalise\n",
    "    flat[filter] = flat[filter]/np.mean(flat[filter])\n",
    "    # Note in the header what we have done\n",
    "    header['HISTORY'] = 'Combined and normalised flat field'\n",
    "    fits.writeto(redu_dir + filter + 'flat.fits',flat[filter],header,overwrite=True)\n",
    "    print(\"Created normalised flat field in \" + filter + \" filter.\")\n",
    "    print(\"-----------------------------------------------------\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Normalised Flat Field Frames\n",
    "\n",
    "It is wise to check the normalised flat field frames using DS9 or similar tool.  Most pixel values should be very close to 1.0.  A typical B-band normalised flat field is shown as an example.\n",
    "<p>\n",
    "<img src=./images/DS9-FlatField.png>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flat Field Data Frames\n",
    "\n",
    "Flat fielding data is an essential step in the data reduction to calibrate the relative sensitivies of each pixel.  First the data files will be sorted based on their filters, then each frame divided by the normalised flat field file in the appropriate filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our file lists will again be contained in a dictionary called flist\n",
    "flist = {}\n",
    "\n",
    "for filter in filters:\n",
    "    # Create an empty list for the file names\n",
    "    flist[filter] = []\n",
    "\n",
    "# Make list of all bias subracted data files\n",
    "datalist = glob(datadir + '*.fits')\n",
    "\n",
    "# Sort files into lists based on the filter used\n",
    "for ifile in datalist:\n",
    "    # Read the header for each file\n",
    "    hdr = fits.getheader(ifile)\n",
    "    # Read which filter this file was taken with\n",
    "    filt = hdr['FILTNAM']\n",
    "    # Loop through each of the filters in our filter set\n",
    "    for filter in filters:\n",
    "        # If the filter listed in the header matches the filter for this array, add the file to the array\n",
    "        if filt == filter:\n",
    "            flist[filter].append(ifile)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each filter, we're going to divide the Data by the Master Flat.\n",
    "# We'll use another dictionary to keep track of the data stacks for each filter\n",
    "data_stack = {}\n",
    "\n",
    "for filter in filters:\n",
    "    # Initialise the stack for this filter\n",
    "    data_stack[filter] = []\n",
    "    \n",
    "    # Read in each file in this filter and divide by the relevant flat\n",
    "    for file in tqdm(flist[filter]):\n",
    "        data,header = fits.getdata(file,header=True)\n",
    "        dataout = data / flat[filter]\n",
    "        # Note in the header what we have done\n",
    "        header['HISTORY'] = 'Flat Fielded'\n",
    "        # Create the output filename\n",
    "        ofile = file[:-5]+ '_ff.fits'         \n",
    "        # And write out the file\n",
    "        fits.writeto(ofile,dataout,header)\n",
    "        # Move bias subtracted images to archive\n",
    "        data = []\n",
    "        header = []\n",
    "        basename = os.path.basename(file)\n",
    "        os.rename(file,archivedir+basename)\n",
    "    \n",
    "    print(\"Flatfielded data frames in \" + filter + \" filter.\")\n",
    "    print(\"-----------------------------------------------------\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Flat Fielded Images\n",
    "\n",
    "It is highly recommended to examine all the images after flat fielding to be sure that the flat field correction has been done propertly.  The image below shows a properly flat fielded image.\n",
    "<p>\n",
    "<img src=./images/DS9-FlatFieldedData.png>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Known Bad Columns in Nickel CCD2 Images\n",
    "\n",
    "The Nickel CCD2 detector has a number of known bad columns (easily seen in the flat fielded image above).  These columns can be \"fixed\" by replacing them with the mean values of neighboring columns.  First a bad pixel pixel mask is made highlighting the known bad columns.  Then for each bad pixel, the mean of the surrounding good pixels is calculated and replaces the bad pixel.   This procedure is somewhat time consuming, so be patient while it runs.  Do not be alarmed if it gives a warning about converting mask elements to nan, as it still works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Procedure to fix known bad columns in CCD2 images.  2016 Oct 2 E. Gates\n",
    "\n",
    "# Create list of flat fielded data\n",
    "datalist = glob(datadir + '*.fits')\n",
    "\n",
    "# _bp in output file name stands for bad pixel corrected\n",
    "#dataout = [i[:-5]+ '_bp.fits' for i in datain]\n",
    "\n",
    "#n=len(datain)\n",
    "# size of box for area around bad pixel to be averaged\n",
    "s=2\n",
    "\n",
    "# read in one image to get image size for bad pixel mask\n",
    "data,header=fits.getdata(datalist[0],header=True)\n",
    "\n",
    "# make bad pixel mask\n",
    "mask=np.ma.make_mask(data,copy=True,shrink=True,dtype=bool)\n",
    "mask[:,:]=False\n",
    "mask[:,255:257]=True\n",
    "mask[:,783:785]=True\n",
    "mask[:,1001:1003]=True\n",
    "\n",
    "# loop for all the data bad pixel correction \n",
    "# Progress bar because this can take a looong time\n",
    "for file in tqdm(datalist):\n",
    "    data,header=fits.getdata(file,header=True)\n",
    "    mdata=np.ma.masked_array(data,mask=mask,fill_value=np.nan)\n",
    "    dataFixed=data.copy()\n",
    "    for i in range(0,mdata.shape[0]):\n",
    "        for j in range(0,mdata.shape[1]):\n",
    "            if math.isnan(mdata[i,j]):\n",
    "                x1=i-s\n",
    "                x2=i+s+1\n",
    "                y1=j-s\n",
    "                y2=j+s+1\n",
    "                if x1<0:\n",
    "                    x1=0\n",
    "                if x2>mdata.shape[0]:\n",
    "                    x2=mdata.shape[0]\n",
    "                if y1<0:\n",
    "                    y1=0\n",
    "                if y2>mdata.shape[1]:\n",
    "                    y2=mdata.shape[1]\n",
    "                dataFixed[i,j]=np.mean(mdata[x1:x2,y1:y2])\n",
    "    header['HISTORY']='Bad columns replaced'\n",
    "    ofile = file[:-5]+ '_bp.fits'\n",
    "    fits.writeto(ofile,dataFixed,header)\n",
    "    # Move the now no longer needed files to archivedir\n",
    "    data = []\n",
    "    header = []\n",
    "    mdata = []\n",
    "    basename=os.path.basename(file)\n",
    "    os.rename(file,archivedir+basename)\n",
    "\n",
    "print(\"Fixed bad columns in Data frames.\")\n",
    "print(\"-----------------------------------------------------\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Bad Pixel Corrected Images\n",
    "\n",
    "As always, it is good to check the pixel corrected images using DS9 or other image display tool.  You can see in the image below that the bad columns were fixed reasonably well.\n",
    "<p>\n",
    "<img src=./images/DS9-BadColFix.png>\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmic Ray Removal\n",
    "\n",
    "While the data probably look very good at this point, there are likely many cosmic rays contaminating the data.  Removing all cosmic rays with software is difficult, but there are scripts that do a pretty good job.  In this case we'll use the python module astroscrappy to do cosmic ray rejection.  If you don't have astroscrappy installed, you'll want to install it using pip:\n",
    "\n",
    "pip install astroscrappy\n",
    "\n",
    "Note, it is not unusual to have to hand remove cosmic rays that are contaminating key pixels for data analysis, but that won't be covered in this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astroscrappy\n",
    "# Make a list of all the reduced data files\n",
    "datalist = glob(datadir + '*.fits')\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "for file in tqdm(datalist):\n",
    "    data,header=fits.getdata(file,header=True)\n",
    "    data_fixed = data.copy()\n",
    "    mask = np.ma.make_mask(data,copy=True,shrink=True, dtype=np.bool_)\n",
    "    mask[:,:] = False\n",
    "    crmask,dataCR = astroscrappy.detect_cosmics(data_fixed,inmask=mask,cleantype='medmask')\n",
    "    header['HISTORY'] = 'CR and bad pixels fixed with astroscrappy'\n",
    "    ofile = file[:-5]+ '_crj.fits'\n",
    "    fits.writeto(ofile,dataCR,header)\n",
    "    # Move the now no longer needed files to archivedir\n",
    "    data = []\n",
    "    header = []\n",
    "    basename=os.path.basename(file)\n",
    "    os.rename(file,archivedir+basename) \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Completed Cosmic Ray Removal using astroscrappy.\")\n",
    "print(\"-----------------------------------------------------\") \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Final Images\n",
    "\n",
    "We have reached the end of the basic data reduction procedure where we have performed overscan subtraction, bias subtraction, flat field correction, removed the bad pixels in the CCD, and replaced cosmic ray hits. The saved final images can now be analyzed for whatever science goal is desired, e.g. astrometry or photometry.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Python Resources and Tutorials\n",
    "\n",
    "Python4Astronomers<br>\n",
    "http://python4astronomers.github.io/intro/intro.html\n",
    "\n",
    "AstroPython Tutorials<br>\n",
    "http://www.astropython.org/tutorials/\n",
    "\n",
    "astropy Tutorials<br>\n",
    "http://www.astropy.org/astropy-tutorials/\n",
    "\n",
    "Python for Astronomers<br>\n",
    "http://www.iac.es/sieinvens/siepedia/pmwiki.php?n=HOWTOs.EmpezandoPython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Three Color Images with DS9\n",
    "\n",
    "Now that you have the data reduced, you can make a pretty three color image.  Basic usage of DS9 RGB frames is described in the following video.  \n",
    "\n",
    "https://www.youtube.com/watch?v=G77RcsAfMGM\n",
    "\n",
    "\n",
    "# Making Three Color Images with GIMP \n",
    "\n",
    "Basic tutorial to make a three color images with GIMP.\n",
    "\n",
    "https://www.youtube.com/watch?v=56-ZaZbA3S0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Data\n",
    "\n",
    "Imexam is a convient tool based on IRAF IMEXAMINE.  One can do aperture photometry, radial profile plots, FWHM measurements, etc. with this tool.   Instructions for installation and use are available on-line at https://imexam.readthedocs.io/en/0.9.1/\n",
    "\n",
    "Other photometry tools are part of the photutils python package (in fact some of the imexam procedures require photoutils).\n",
    "https://photutils.readthedocs.io/en/stable/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
